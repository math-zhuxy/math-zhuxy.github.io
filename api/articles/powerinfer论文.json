{"title":"PowerInfer论文","uid":"1150f8d04ea2174e83f737731d4cd296","slug":"powerinfer论文","date":"2024-11-19T12:30:31.000Z","updated":"2024-11-30T03:00:13.521Z","comments":true,"path":"api/articles/powerinfer论文.json","keywords":null,"cover":[],"content":"<p><a href=\"https://dl.acm.org/doi/pdf/10.1145/3694715.3695964\">论文原文链接</a><br><a href=\"https://github.com/SJTU-IPADS/PowerInfer\">论文github链接</a></p>\n<h1 id=\"论文结构\"><a href=\"#论文结构\" class=\"headerlink\" title=\"论文结构\"></a>论文结构</h1><h2 id=\"1-引言\"><a href=\"#1-引言\" class=\"headerlink\" title=\"1. 引言\"></a>1. 引言</h2><ul>\n<li>讨论了当前大语言模型的推理需求与挑战，尤其是在消费级GPU上运行模型的难点： <ul>\n<li>大语言模型的内存需求巨大，远超消费级GPU的容量。</li>\n<li>数据中心部署的方法通常无法满足本地部署的低延迟需求。</li>\n</ul>\n</li>\n<li>提出PowerInfer，通过利用神经元激活的稀疏性和局部性，减少GPU内存需求，并提升推理效率。</li>\n</ul>\n<h2 id=\"2-背景与动机\"><a href=\"#2-背景与动机\" class=\"headerlink\" title=\"2. 背景与动机\"></a>2. 背景与动机</h2><ul>\n<li>介绍了LLM推理的过程和架构，包括自注意力模块和多层感知机模块(MLP)。</li>\n<li>阐述了激活稀疏性： <ul>\n<li>在推理过程中，仅有少部分神经元对输出结果有显著贡献（称为“热神经元”），其余神经元（称为“冷神经元”）根据输入动态激活。</li>\n<li>激活稀疏性可以通过统计学方法预测，为加速推理提供可能。</li>\n</ul>\n</li>\n<li>回顾现有的模型卸载技术（如CPU-GPU混合部署），指出其面临的局部性错配问题。</li>\n</ul>\n<h2 id=\"3-LLM推理中的局部性洞察\"><a href=\"#3-LLM推理中的局部性洞察\" class=\"headerlink\" title=\"3. LLM推理中的局部性洞察\"></a>3. LLM推理中的局部性洞察</h2><ul>\n<li>提出两大洞察： <ul>\n<li>洞察1：激活的幂律分布：少量热神经元占大多数激活频率，可利用其特性优化推理过程。</li>\n<li>洞察2：在CPU上的快速计算：小批量推理时，直接在CPU上处理部分神经元比数据从CPU传输到GPU更高效。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-PowerInfer框架\"><a href=\"#4-PowerInfer框架\" class=\"headerlink\" title=\"4. PowerInfer框架\"></a>4. PowerInfer框架</h2><ul>\n<li>架构概览： <ul>\n<li>结合GPU和CPU的推理引擎，热神经元加载到GPU中，而冷神经元在CPU上计算。</li>\n<li>离线部分分析神经元的激活模式，将其分类为“热”或“冷”。</li>\n<li>在线部分预测每次推理中将被激活的神经元，仅处理必要的计算任务。</li>\n</ul>\n</li>\n<li>具体设计： <ul>\n<li>利用自适应预测器优化热神经元的存储。</li>\n<li>引入稀疏神经元操作符，提高CPU和GPU的计算效率。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"5-神经元感知推理引擎\"><a href=\"#5-神经元感知推理引擎\" class=\"headerlink\" title=\"5. 神经元感知推理引擎\"></a>5. 神经元感知推理引擎</h2><ul>\n<li>详细描述预测器的构建及其对推理计算负载的减少。</li>\n<li>描述GPU-CPU混合执行模式和稀疏操作符的实现，突出其如何绕过非活跃神经元的冗余计算。</li>\n</ul>\n<h2 id=\"6-神经元分配策略\"><a href=\"#6-神经元分配策略\" class=\"headerlink\" title=\"6. 神经元分配策略\"></a>6. 神经元分配策略</h2><ul>\n<li>提出了一种基于整数线性规划（ILP）的优化方法： <ul>\n<li>通过离线分析，最大化GPU上加载的神经元对推理效率的贡献。</li>\n<li>通过分配策略，平衡GPU和CPU之间的计算负载，并减少CPU-GPU数据传输。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"7-实现细节\"><a href=\"#7-实现细节\" class=\"headerlink\" title=\"7. 实现细节\"></a>7. 实现细节</h2><ul>\n<li>PowerInfer在现有的llama.cpp开源框架上进行了扩展，添加了4200行C++和CUDA代码，以及400行Python代码，用于离线分析与策略生成。</li>\n</ul>\n<h2 id=\"8-评估\"><a href=\"#8-评估\" class=\"headerlink\" title=\"8. 评估\"></a>8. 评估</h2><ul>\n<li>实验环境： <ul>\n<li>评估使用了两种硬件配置：高端（如RTX 4090）和低端（如RTX 2080Ti）PC。</li>\n<li>模型包括多个主流LLM（如OPT、LLaMA2、Falcon）和不同激活函数的模型。</li>\n</ul>\n</li>\n<li>性能表现： <ul>\n<li>PowerInfer在消费级GPU上实现了大幅的推理加速，相比于llama.cpp最多可达到11.69倍的速度提升。</li>\n<li>对比SpecInfer等基线框架，在不同输入输出长度下也表现出明显优势。</li>\n<li>性能优势随着输出令牌数增加而进一步扩大。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"9-结论\"><a href=\"#9-结论\" class=\"headerlink\" title=\"9. 结论\"></a>9. 结论</h2><ul>\n<li>PowerInfer通过利用神经元激活的局部性和稀疏性，在消费级硬件上实现了高效的大语言模型推理，展现出其在实际应用场景中的潜力。</li>\n</ul>\n<h1 id=\"核心部分讲解-4-5-6部分\"><a href=\"#核心部分讲解-4-5-6部分\" class=\"headerlink\" title=\"核心部分讲解(4,5,6部分)\"></a>核心部分讲解(4,5,6部分)</h1><h2 id=\"4-PowerInfer框架总览\"><a href=\"#4-PowerInfer框架总览\" class=\"headerlink\" title=\"4. PowerInfer框架总览\"></a>4. PowerInfer框架总览</h2><p>PowerInfer通过结合GPU和CPU的计算能力，并利用神经元的激活稀疏性，显著提升大语言模型（LLM）的推理性能。</p>\n<h2 id=\"4-1-框架架构\"><a href=\"#4-1-框架架构\" class=\"headerlink\" title=\"4.1 框架架构\"></a>4.1 框架架构</h2><p>PowerInfer由离线和在线两个阶段组成：</p>\n<ol>\n<li>离线阶段：</li>\n</ol>\n<ul>\n<li>任务：分析神经元激活的统计特性，制定神经元分配策略。</li>\n<li>步骤： <ul>\n<li>使用通用数据集，运行离线推理分析工具，记录各层神经元的激活频率。</li>\n<li>分类神经元为热神经元（高频激活）和冷神经元（低频激活）。</li>\n<li>构建神经元分配策略，将热神经元分配到GPU，冷神经元分配到CPU。</li>\n</ul>\n</li>\n<li>输出：分配策略和预测器参数。</li>\n</ul>\n<ol start=\"2\">\n<li>在线阶段：</li>\n</ol>\n<ul>\n<li>任务：根据离线生成的策略，在推理时动态加载激活神经元，并高效执行推理。</li>\n<li>步骤： <ul>\n<li>GPU加载热神经元，CPU处理冷神经元。</li>\n<li>使用预测器动态预测当前推理中需要激活的神经元，仅计算激活部分。</li>\n<li>通过CPU-GPU混合计算完成推理。</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-2-执行流程\"><a href=\"#4-2-执行流程\" class=\"headerlink\" title=\"4.2 执行流程\"></a>4.2 执行流程</h2><p>PowerInfer的推理流程可以分为以下几步：</p>\n<ol>\n<li>离线阶段生成热&#x2F;冷神经元分配表。</li>\n<li>推理过程中，输入数据会被送入在线预测器。</li>\n<li>预测器动态预测哪些神经元会被激活：</li>\n</ol>\n<ul>\n<li>GPU计算激活的热神经元。</li>\n<li>CPU计算激活的冷神经元。</li>\n</ul>\n<ol start=\"4\">\n<li>GPU将CPU的计算结果整合并完成推理输出。</li>\n</ol>\n<h2 id=\"5-神经元感知推理引擎-1\"><a href=\"#5-神经元感知推理引擎-1\" class=\"headerlink\" title=\"5. 神经元感知推理引擎\"></a>5. 神经元感知推理引擎</h2><p>PowerInfer的核心是高效的推理引擎，包含自适应稀疏预测器、GPU-CPU混合执行模型、稀疏操作符等关键组件。</p>\n<h2 id=\"5-1-自适应稀疏预测器\"><a href=\"#5-1-自适应稀疏预测器\" class=\"headerlink\" title=\"5.1 自适应稀疏预测器\"></a>5.1 自适应稀疏预测器</h2><h3 id=\"任务：\"><a href=\"#任务：\" class=\"headerlink\" title=\"任务：\"></a>任务：</h3><ul>\n<li>通过预测当前推理任务中被激活的神经元，减少冗余计算。</li>\n</ul>\n<h3 id=\"公式与实现：\"><a href=\"#公式与实现：\" class=\"headerlink\" title=\"公式与实现：\"></a>公式与实现：</h3><ol>\n<li>预测器的设计：</li>\n</ol>\n<ul>\n<li>预测器是一个小型的多层感知机（MLP），由输入层、隐藏层和输出层组成： <ul>\n<li>输入层：输入数据向量。</li>\n<li>隐藏层：可调节大小，用于平衡预测精度和内存占用。</li>\n<li>输出层：预测哪些神经元会被激活。</li>\n</ul>\n</li>\n</ul>\n<ol start=\"2\">\n<li>预测器大小的动态调整：</li>\n</ol>\n<ul>\n<li>输入稀疏性和偏斜性影响预测器大小： <ul>\n<li>层级稀疏性高时，预测任务更简单，可以使用小型预测器。</li>\n<li>激活分布高度偏斜时（少量神经元频繁激活），更易用小型预测器。</li>\n</ul>\n</li>\n</ul>\n<ol start=\"3\">\n<li>公式：隐藏层调整：</li>\n</ol>\n<ul>\n<li>假设层级稀疏性为S（稀疏率），预测器大小由下式确定： P&#x3D;f(S,K)其中，P为预测器参数量，K表示层激活分布的偏斜性。</li>\n<li>通过迭代调整隐藏层神经元数量，确保模型精度损失小于0.1%。</li>\n</ul>\n<h2 id=\"5-2-神经元管理\"><a href=\"#5-2-神经元管理\" class=\"headerlink\" title=\"5.2 神经元管理\"></a>5.2 神经元管理</h2><h3 id=\"任务：-1\"><a href=\"#任务：-1\" class=\"headerlink\" title=\"任务：\"></a>任务：</h3><ul>\n<li>在GPU和CPU之间合理分配神经元，以实现高效推理。</li>\n</ul>\n<h3 id=\"设计与实现：\"><a href=\"#设计与实现：\" class=\"headerlink\" title=\"设计与实现：\"></a>设计与实现：</h3><ol>\n<li>神经元表：</li>\n</ol>\n<ul>\n<li>两张表分别存储在GPU和CPU内存中，用于记录每个神经元的位置和状态。</li>\n<li>表的大小非常小，例如OPT-175B模型仅需9MB内存。</li>\n</ul>\n<ol start=\"2\">\n<li>神经元映射：</li>\n</ol>\n<ul>\n<li>在进行矩阵-向量乘法时，每个神经元根据神经元表映射到对应的存储位置，确保计算的正确性。</li>\n</ul>\n<h2 id=\"5-3-GPU-CPU混合执行\"><a href=\"#5-3-GPU-CPU混合执行\" class=\"headerlink\" title=\"5.3 GPU-CPU混合执行\"></a>5.3 GPU-CPU混合执行</h2><h3 id=\"流程：\"><a href=\"#流程：\" class=\"headerlink\" title=\"流程：\"></a>流程：</h3><ol>\n<li>全局计算图DAG（有向无环图）：</li>\n</ol>\n<ul>\n<li>推理阶段，构建一个DAG图，节点表示计算操作，边表示数据依赖。</li>\n<li>DAG图存储在CPU内存中，供GPU和CPU的执行器访问。</li>\n</ul>\n<ol start=\"2\">\n<li>计算任务分配：</li>\n</ol>\n<ul>\n<li>GPU负责计算热神经元，CPU负责计算冷神经元。</li>\n<li>任务分配通过检查依赖关系和内存使用情况实现。</li>\n</ul>\n<ol start=\"3\">\n<li>公式：CPU-GPU同步时间：<br><img src=\"/../figure/powerinfer_time.png\"></li>\n</ol>\n<h2 id=\"5-4-神经元感知操作符\"><a href=\"#5-4-神经元感知操作符\" class=\"headerlink\" title=\"5.4 神经元感知操作符\"></a>5.4 神经元感知操作符</h2><ul>\n<li>传统稀疏矩阵计算库（如cuSPARSE）效率不高。</li>\n<li>PowerInfer设计了基于向量操作的神经元感知操作符： <ul>\n<li>GPU操作符：每个线程块独立计算神经元，避免同步开销。</li>\n<li>CPU操作符：利用硬件指令集（如AVX2），加速向量计算。</li>\n</ul>\n</li>\n</ul>\n<ol start=\"4\">\n<li>神经元分配策略<br>通过优化算法将神经元合理分配到GPU和CPU。</li>\n</ol>\n<h2 id=\"6-1-离线分析\"><a href=\"#6-1-离线分析\" class=\"headerlink\" title=\"6.1 离线分析\"></a>6.1 离线分析</h2><p>任务：</p>\n<ul>\n<li>记录每个神经元的激活频率，统计热&#x2F;冷神经元的比例。</li>\n<li>公式：神经元影响度量： vi&#x3D;fi 其中，vi是神经元的影响值，fi是离线分析得到的激活频率。</li>\n</ul>\n<h2 id=\"6-2-神经元分配建模\"><a href=\"#6-2-神经元分配建模\" class=\"headerlink\" title=\"6.2 神经元分配建模\"></a>6.2 神经元分配建模</h2><p>目标：<br>最大化GPU上加载神经元的总影响值。<br><img src=\"/../figure/powerinfer_formation.png\"><br>3. 整数线性规划（ILP）：</p>\n<ul>\n<li>将所有约束转化为线性方程，使用ILP求解器确定神经元分配方案。</li>\n</ul>\n<h2 id=\"6-3-批处理策略\"><a href=\"#6-3-批处理策略\" class=\"headerlink\" title=\"6.3 批处理策略\"></a>6.3 批处理策略</h2><ul>\n<li>为减少ILP求解时间，将每层神经元按影响值分组（每组64个），显著降低计算复杂度。</li>\n</ul>\n","text":"论文原文链接论文github链接 论文结构1. 引言 讨论了当前大语言模型的推理需求与挑战，尤其是在消费级GPU上运行模型的难点： 大语言模型的内存需求巨大，远...","permalink":"/post/powerinfer论文","photos":[],"count_time":{"symbolsCount":"3k","symbolsTime":"3 mins."},"categories":[],"tags":[{"name":"论文阅读","slug":"论文阅读","count":1,"path":"api/tags/论文阅读.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84\"><span class=\"toc-text\">论文结构</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-%E5%BC%95%E8%A8%80\"><span class=\"toc-text\">1. 引言</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-%E8%83%8C%E6%99%AF%E4%B8%8E%E5%8A%A8%E6%9C%BA\"><span class=\"toc-text\">2. 背景与动机</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#3-LLM%E6%8E%A8%E7%90%86%E4%B8%AD%E7%9A%84%E5%B1%80%E9%83%A8%E6%80%A7%E6%B4%9E%E5%AF%9F\"><span class=\"toc-text\">3. LLM推理中的局部性洞察</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-PowerInfer%E6%A1%86%E6%9E%B6\"><span class=\"toc-text\">4. PowerInfer框架</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-%E7%A5%9E%E7%BB%8F%E5%85%83%E6%84%9F%E7%9F%A5%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E\"><span class=\"toc-text\">5. 神经元感知推理引擎</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6-%E7%A5%9E%E7%BB%8F%E5%85%83%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5\"><span class=\"toc-text\">6. 神经元分配策略</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82\"><span class=\"toc-text\">7. 实现细节</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#8-%E8%AF%84%E4%BC%B0\"><span class=\"toc-text\">8. 评估</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#9-%E7%BB%93%E8%AE%BA\"><span class=\"toc-text\">9. 结论</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E6%A0%B8%E5%BF%83%E9%83%A8%E5%88%86%E8%AE%B2%E8%A7%A3-4-5-6%E9%83%A8%E5%88%86\"><span class=\"toc-text\">核心部分讲解(4,5,6部分)</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-PowerInfer%E6%A1%86%E6%9E%B6%E6%80%BB%E8%A7%88\"><span class=\"toc-text\">4. PowerInfer框架总览</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-1-%E6%A1%86%E6%9E%B6%E6%9E%B6%E6%9E%84\"><span class=\"toc-text\">4.1 框架架构</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-2-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B\"><span class=\"toc-text\">4.2 执行流程</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-%E7%A5%9E%E7%BB%8F%E5%85%83%E6%84%9F%E7%9F%A5%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E-1\"><span class=\"toc-text\">5. 神经元感知推理引擎</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-1-%E8%87%AA%E9%80%82%E5%BA%94%E7%A8%80%E7%96%8F%E9%A2%84%E6%B5%8B%E5%99%A8\"><span class=\"toc-text\">5.1 自适应稀疏预测器</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BB%BB%E5%8A%A1%EF%BC%9A\"><span class=\"toc-text\">任务：</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%85%AC%E5%BC%8F%E4%B8%8E%E5%AE%9E%E7%8E%B0%EF%BC%9A\"><span class=\"toc-text\">公式与实现：</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-2-%E7%A5%9E%E7%BB%8F%E5%85%83%E7%AE%A1%E7%90%86\"><span class=\"toc-text\">5.2 神经元管理</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BB%BB%E5%8A%A1%EF%BC%9A-1\"><span class=\"toc-text\">任务：</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%EF%BC%9A\"><span class=\"toc-text\">设计与实现：</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-3-GPU-CPU%E6%B7%B7%E5%90%88%E6%89%A7%E8%A1%8C\"><span class=\"toc-text\">5.3 GPU-CPU混合执行</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B5%81%E7%A8%8B%EF%BC%9A\"><span class=\"toc-text\">流程：</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-4-%E7%A5%9E%E7%BB%8F%E5%85%83%E6%84%9F%E7%9F%A5%E6%93%8D%E4%BD%9C%E7%AC%A6\"><span class=\"toc-text\">5.4 神经元感知操作符</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6-1-%E7%A6%BB%E7%BA%BF%E5%88%86%E6%9E%90\"><span class=\"toc-text\">6.1 离线分析</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6-2-%E7%A5%9E%E7%BB%8F%E5%85%83%E5%88%86%E9%85%8D%E5%BB%BA%E6%A8%A1\"><span class=\"toc-text\">6.2 神经元分配建模</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6-3-%E6%89%B9%E5%A4%84%E7%90%86%E7%AD%96%E7%95%A5\"><span class=\"toc-text\">6.3 批处理策略</span></a></li></ol></li></ol>","author":{"name":"Math-zhuxy","slug":"blog-author","avatar":"https://sse-market-source-1320172928.cos.ap-guangzhou.myqcloud.com/src/images/uploads/1728918801765528128_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20241014222719.jpg","link":"/","description":"Blood of the First Men, drawn by the sword.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"MST","uid":"ac69b4c1992072269abea91444d26b98","slug":"MST","date":"2024-11-21T01:56:04.000Z","updated":"2024-11-21T08:21:55.038Z","comments":true,"path":"api/articles/MST.json","keywords":null,"cover":null,"text":"MST定义设 $G=(V,E)$ 是一个无向连通图，$E$ 中的每个权值 $c(u,v)$ ，称为 $(u,v)$ 的边长。图 $G$ 的生成树( $n-1$ ...","permalink":"/post/MST","photos":[],"count_time":{"symbolsCount":"3.5k","symbolsTime":"3 mins."},"categories":[],"tags":[{"name":"算法","slug":"算法","count":19,"path":"api/tags/算法.json"}],"author":{"name":"Math-zhuxy","slug":"blog-author","avatar":"https://sse-market-source-1320172928.cos.ap-guangzhou.myqcloud.com/src/images/uploads/1728918801765528128_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20241014222719.jpg","link":"/","description":"Blood of the First Men, drawn by the sword.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"配置极光主题","uid":"a8b6b9bbf4052440f27a448efb8e5b47","slug":"极光主题配置","date":"2024-11-17T14:44:01.000Z","updated":"2024-11-17T15:09:08.459Z","comments":true,"path":"api/articles/极光主题配置.json","keywords":null,"cover":null,"text":"下载HEXO首先要先下载npm，在Node.js官网下载LTS版本到本地。下载好后，在命令行中输入： 1npm -v 来查看版本如果之前已经下载过了，但是版本落...","permalink":"/post/极光主题配置","photos":[],"count_time":{"symbolsCount":"2.6k","symbolsTime":"2 mins."},"categories":[],"tags":[{"name":"配置","slug":"配置","count":2,"path":"api/tags/配置.json"}],"author":{"name":"Math-zhuxy","slug":"blog-author","avatar":"https://sse-market-source-1320172928.cos.ap-guangzhou.myqcloud.com/src/images/uploads/1728918801765528128_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20241014222719.jpg","link":"/","description":"Blood of the First Men, drawn by the sword.","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}